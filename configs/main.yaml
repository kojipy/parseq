defaults:
  - _self_
  - model: parseq
  # - charset: 94_full

model:
  _convert_: all
  img_size: [ 32, 512 ]  # [ height, width ]
  max_label_length: 50
  # The ordering in charset_train matters. It determines the token IDs assigned to each character.
  target_signs_file: data/target_hittite_cuneiform_signs.json
  batch_size: 4
  weight_decay: 0.0
  warmup_pct: 0.075  # equivalent to 1.5 epochs of warm up

data:
  _target_: strhub.data.module.AbgalDataModule
  synth_root_dir: data
  real_root_dir: data
  train_first_idx: 0
  train_last_idx: 100
  valid_first_idx: 101
  valid_last_idx: 150
  img_height: 32
  img_width: 512
  batch_size: 4
  num_workers: 8

trainer:
  _target_: pytorch_lightning.Trainer
  _convert_: all
  val_check_interval: 5
  #max_steps: 169680  # 20 epochs x 8484 steps (for batch size = 384, real data)
  max_epochs: 20
  gradient_clip_val: 20
  # gpus: 2

ckpt_path: null
pretrained: null

hydra:
  output_subdir: config
  run:
    dir: outputs/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: multirun/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.override_dirname}



